<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
readr_example("mtcars.csv")
# import and export comma-delimited files
mtcars <- read_csv(readr_example("mtcars.csv"))
head(mtcars)
write_csv(mtcars, "data/spreadsheets/mtcars-comma.csv")
# import and export semi-colon delimited files (Germans!)
write_delim(mtcars, delim = ";", path = "data/spreadsheets/mtcars-semicolon.csv")
mtcars <- read_csv2("data/spreadsheets/mtcars-semicolon.csv")
head(mtcars)
## FOR R NERDS
# why readr, not base R?
# readr is much faster (up to 10x)
# strings remain strings by default
# automatically parse common date/time formats
# progress bar if needed
# importing Stata files
library(haven)
dir.create("data/stata")
write_dta(mtcars, "data/stata/mtcars.dta")
mtcars_stata <- read_dta("data/stata/mtcars.dta")
## FOR R NERDS
# why not use functions from foreign package?
# haven works with binary files from newer Stata versions, too
# retains value/variable labels
# ************************************************
# PIPING -----------------------------------------
# what is piping?
# structures sequences of data operations as "pipes, i.e. left-to-right (as opposed to from the inside and out)
# serves the natural way of reading ("do this, then this, then this, ...")
# avoids nested function calls
# improves "cognitive performance" of code writers and readers
# minimizes the need for local variables and function definitions
# why name "magrittr"?
browseURL("https://upload.wikimedia.org/wikipedia/en/b/b9/MagrittePipe.jpg")
# traditional way of writing code
dat <- babynames
dim(dat)
dat_filtered <- filter(dat, name == "Kim")
dat_grouped <- group_by(dat_filtered, year, sex)
dat_sum <- summarize(dat_grouped, total = sum(n))
qplot(year, total, color = sex, data = dat_sum, geom = "line") +
ggtitle('People named "Kim"')
# traditional, even more awkward way of writing code
dat <- summarize(group_by(filter(babynames, name == "Kim"), year, sex), total = sum(n))
install.packages("dplyr")
babynames %>%
filter(name %>% equals("Kim")) %>%
group_by(year, sex) %>%
summarize(total = sum(n)) %>%
qplot(year, total, color = sex, data = ., geom = "line") %>%
add(ggtitle('People named "Kim"')) %>%
print
# syntax and vocabulary
# by default, the left-hand side (LHS) will be piped in as the first argument of the function appearing on the right-hand side (RHS)
# %>% may be used in a nested fashion, e.g. it may appear in expressions within arguments. This is used in the mpg to kpl conversion
# when the LHS is needed at a position other than the first, one can use the dot,'.', as placeholder
# whenever only one argument is needed--the LHS--, the parentheses can be omitted
# ************************************************
# MANIPULATING DATA FRAMES -----------------------
# dplyr, by Hadley Wickham, provides a flexible grammar of data manipulation
# three main goals
# identify the most important data manipulation verbs and make them easy to use from R
# provide fast performance for in-memory data
# use the same interface to work with data no matter where it's stored, whether in a data frame, data table or database.
# get data from nycflights13 package
# source: [https://goo.gl/8hlrJb]
# info about the dataset:
browseURL("http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&Link=0")
dat <- flights %>% as.data.frame
head(dat)
library(magrittr)
browseURL("http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&Link=0")
dat <- flights %>% as.data.frame
head(dat)
library("dplyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
dat <- flights %>% as.data.frame
library("nycflights13", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
dat <- flights %>% as.data.frame
head(dat)
library(nycflights13)
names(flights)
dim(flights)
sapply(flights, class)
dim(flights)
?class
str(flights)
table(flight$origin)
table(flights$origin)
selected_flights <- select(flights, day=1)
selected_flights
selected_flights <- filter(flights, day=1)
selected_flights <- filter(flights, day==1)
filter(flights, day==1)
=======
source("packages.r")
source("functions.r")
dat <- flights %>% as.data.frame
head(dat)
destinations <- group_by(dat, dest)
summarize(destinations, planes = n_distinct(tailnum))
summarize_each(destinations, funs(mean))
head(destinations)
table(dat$dest)
destinations <- group_by(dat, dest)
summarize_each(dat, funs(mean))
summarize_each(dat, funs(mean, median))
?summarize_each
summarize_each_(dat, funs(mean), distance, air_time)
summarize_each_(dat, funs(mean), vars = c(distance, air_time))
summarize_each_(dat, funs(mean), vars = c("distance", "air_time"))
summarize_each_(dat, funs(mean, na.rm = TRUE), vars = c("distance", "air_time"))
summarize_each_(dat, funs(mean), vars = c("distance", "air_time"), na.rm = TRUE)
summarize_each_(dat, funs(mean), vars = c("distance", "air_time"))
dat
head(dat)
summarize_each_(dat, funs(mean), vars = c("distance", "arr_delay"))
summarize_each_(dat, funs_(mean, na.rm = T), vars = c("distance", "arr_delay"))
summarize_each_(dat, funs_(mean, list(na.rm = T)), vars = c("distance", "arr_delay"))
summarize_each_(dat, funs_(list(mean), list(na.rm = T)), vars = c("distance", "arr_delay"))
summarize_each_(dat, funs(m1 = mean(., na.rm = TRUE)), vars = c("distance", "arr_delay"))
summarize_each_(dat, funs(mean = mean(., na.rm = TRUE)), vars = c("distance", "arr_delay"))
<<<<<<< HEAD
>>>>>>> simonmunzert/master
=======
x <- c(4,8,15,16,23,42)
x
mode(x)
length(x)
summary(x)
countries <- c("Germany", "France", "Netherlands", "Belgium")
countries
paste("Hello", "world!", sep = " ")
paste("Hello", "world!", sep = ", ")
paste0("Hello", "world!")
paste("Hello", "world!")
c(countries, "Poland")
mode(countries)
length(countries)
summary(countries)
countries
x
x > 15
x == sqrt(225)
(x > 8 & x < 23)
(x > 8 | x < 23)
y <- c(1,10,NA,7,NA,11)
sum(y)
sum(y, na.rm = TRUE)
y
y == NA
is.na(y)
!is.na(y)
!is.na(y)
y*3
?rep
?seq
seq(1, 10, 2)
seq_along(x)
y
countries
seq_along(countries)
rep(c(1, 2, 3), 2)
rep(c(1, 2, 3), 3)
=======
>>>>>>> simonmunzert/master
?rep
rep(c(1, 2, 3), times = 3)
rep(c(1, 2, 3), each = 2)
rep(c(1, 2, 3), each = 2)
vec1 <- c(2, 20, -5, 1, 200)
vec1
vec2 <- seq(1, 5)
vec2
vec2 <- seq(1, 5, 1.5)
vec2
sort(vec1)
sort(vec1, decreasing = FALSE)
sort(vec1, decreasing = TRUE)
order(vec1, decreasing = FALSE)
vec1
order(vec2)
order(vec2)
vec2 <- seq(1, 5)
order(vec2)
order(vec1)
vec1[order(vec1)]
sort(vec1)
vec1[order(vec1)]
vec3 <- c(1,10,NA,7,NA,11)
vec3
vec4 <- vec3[!is.na(vec3)]
vec4
z <- c(1,2,"Bavaria", 4)
z
str(z)
zz <- c(1,2,Bavaria,4,5,6) # error
Bavaria <- 3
zz <- c(1,2,Bavaria,4,5,6)
zz
str(zz)
zz
zzchar <- as.character(zz)
zzchar
zznum <- as.numeric(zzchar)
zznum
zzchar <- "Bavaria"
zzchar
zzchar <- as.character(zz)
zzchar[3] <- "Bavaria"
zzchar
zznum <- as.numeric(zzchar)
zznum
zzchar[2] <- "Berlin"
zzchar
as.numeric(zzchar)
x
zz
xzz <- c(x,zz)
xzz
countries
countries[2]
xzz
xzz[1:6] # xzz[seq(1,6)], xzz[c(1,2,3,4,5,6)]
xzz[c(2, 5, 10)]
xzz[-1]
xzz -1
xzz[-(1, 3)]
xzz[-c(1, 3)]
Hessen
xzz[Hessen]
Hessen <- 4
xzz[Hessen]
seq(0, 10, by = 2)
xzz[seq(0, 10, by = 2)]
xzz[c(TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE, FALSE, FALSE, FALSE)]
y
y[is.na(y)]
y[!is.na(y)]
y
y[y>5 | !is.na(y)]
y[y>5 | is.na(y)]
countries
countries[3] <- "Switzerland"
countries
xzz
xzz[c(1, 3, 5)] <- c(100,110,120)
xzz
xzz_new <- xzz
xzz_new
xzz_new[xzz > 100] <- 1
xzz_new
xzz_new[xzz <= 100] <- 0
xzz_new
x <- seq(0, 20, 4)
x
y <- c(3,3,3,4,4,4,4,5,5,5,5,5)
y
rep(c(3, 4, 5), each = 3)
rep(c(3, 4, 5), c(3, 4, 5))
x
y
z <- c(x[1:5], y[2:12])
z
seq(1, 100) %>% sum()
library(magrittr)
seq(1, 100) %>% sum()
sum(1:100)
101*50
seq(1, 100, 2)
seq(1, 100, 2)^2
seq(1, 100, 2)^2 %>% sum
foo_df <- as.data.frame(matrix(ncol = 6))
foo_df
names(foo_df)
names(foo_df) <- c("hIgHlo", "REPEAT VALUE", "REPEAT VALUE", "% successful (2009)",  "abc@!*", "")
foo_df
janitor::clean_names(foo_df)
make.names(names(foo_df)) # base R solution - not very convincing
c(1:5, 98, 99)
sample(c(1:5, 98, 99))
sample(c(1:5, 98, 99))
sample(c(1:5, 98, 99))
sample(c(1:5, 98, 99), 20, replace = TRUE)
sample(c(1:5, 98, 99), 20, replace = TRUE)
sample(c(1:5, 98, 99), 20, replace = TRUE)
convert_to_NA(sample(c(1:5, 98, 99), 20, replace = TRUE), c(98,99))
source("packages.r")
source("functions.r")
?convert_to_NA
convert_to_NA(sample(c(1:5, 98, 99), 20, replace = TRUE), c(98,99))
foo <- sample(c(1:5, 98, 99), 20, replace = TRUE)
foo
foo[foo == 98 | foo == 99] <- NA
foo
head(mtcars)
table(mtcars$cyl)
janitor::tabyl(mtcars$cyl, show_na = TRUE, sort = TRUE)
janitor::tabyl(mtcars$cyl, show_na = TRUE, sort = TRUE) %>% add_totals_row()
mtcars %$% table(cyl, gear)
mtcars %>% janitor::crosstab(cyl, gear)
mtcars %>% janitor::crosstab(cyl, gear) %>% adorn_crosstab(denom = "row", show_totals = TRUE)
mtcars %>% janitor::crosstab(cyl, gear) %>% adorn_crosstab(denom = "row") %>% add_totals_row()
crosstab(cyl, gear) %>% adorn_crosstab(denom = "row")
mtcars %>% janitor::crosstab(cyl, gear) %>% adorn_crosstab(denom = "row")
mtcars
sum_vector <- vector()
sum_vector
sum_vector <- numeric()
sum_vector
sum_vector <- numeric()
for(i in 1:4) {
sum_vector[i] <- mtcars[,i] %>% sum
}
sum_vector
mtcars[,1]
names(mtcars)
sum_vector <- numeric()
for(i in 1:11) {
sum_vector[i] <- mtcars[,i] %>% sum
}
sum_vector
sum_vector <- numeric()
for(i in 1:12) {
sum_vector[i] <- mtcars[,i] %>% sum
}
sum_vector
str(mtcars)
length(mtcars)
seq_along(mtcars)
ncol(mtcars)
nrow(mtcars)
sum_vector <- numeric()
for(i in 1:ncol(mtcars)) {
sum_vector[i] <- mtcars[,i] %>% sum
}
sum_vector
lapply(mtcars, is.numeric)
sum_vector <- logical()
for(i in 1:ncol(mtcars)) {
sum_vector[i] <- mtcars[,i] %>% is.numeric
}
sum_vector
foo <- lapply(mtcars, is.numeric)
foo
class(foo)
=======
>>>>>>> simonmunzert/master
sapply(mtcars, is.numeric)
foo <- sapply(mtcars, is.numeric)
class(foo)
sapply(mtcars, is.numeric) %>% class
a <- matrix(1:20, nrow = 5)
a
apply(a, 1, mean)
apply(a, 2, mean)
apply(mtcars, 2, mean)
apply(mtcars, 1, mean)
names(mtcars)
apply(mtcars[,1:2], 1, mean)
mtcars[,1:2] %>% apply(1, mean)
apply(mtcars[,1:2], 1, mean)
names(mtcars)
apply(mtcars[,c("gear", "carb")], 1, mean)
select(mtcars, gear, carb)
select(mtcars, gear, carb) %>% apply(1, mean)
?runif
?rnorm
runif(10)
runif(10)
runif(10)
runif(10)
runif(10)
runif(10)
xs <- replicate(5, runif(10), simplify = FALSE)
xs
ws <- replicate(5, rpois(10, 5) + 1, simplify = FALSE)
ws
?weighted.mean
Map(weighted.mean, xs, ws)
Map(weighted.mean, xs, ws) %>% unlist
mtcars
mtcars$mpg
mtcars$mpg %>% class
(current_folder <- getwd())
setwd("/Users/munzerts/github/")
setwd("/Users/munzerts/github/rscraping-hu-2017")
dir.create("data")
dir.create("data/r-data")
# get all pre-compiled data sets
dat <- as.data.frame(data(package = "datasets")$results)
dat$Item %<>% str_replace(" \\(.+\\)", "")
dat
# store data sets in local folder
for (i in 1:50) {
try(df_out <- dat$Item[i] %>% as.character %>% get)
save(df_out, file = paste0("data/r-data/", dat$Item[i], ".RData"))
}
dir()
dir("data/r-data")
filenames <- dir("data/r-data", full.names = TRUE)
filenames
dir("data/r-data", pattern = "US")
dir("data/r-data", pattern = "US", ignore.case = TRUE)
?files
filenames
basename(filenames)
url <- "http://www.mzes.uni-mannheim.de/d7/en/news/media-coverage/ist-die-wahlforschung-in-der-krise-der-undurchschaubare-buerger"
url
basename(url)
dirname(url)
file_inf <- file.info(dir(recursive = F))
file_inf
tools::file_ext(filenames)
filenames
tools::file_ext(dir())
file.exists(filenames)
file.exists("voterfile.RData")
file.exists(filenames)
file.exists()
(foo <- file.choose())
<<<<<<< HEAD
>>>>>>> simonmunzert/master
source("packages.r")
source("functions.r")
url <- "http://www.cses.org/datacenter/module4/module4.htm"
library(xml2)
library(rvest)
page_links <- read_html(url) %>% html_nodes("a") %>% html_attr("href")
survey_pdfs <- str_subset(page_links, "/survey")
dir.create("data/cses-pdfs")
out <- getwd()
baseurl <- "http://www.cses.org"
survey_pdfs
out <- paste0(getwd(), /)
out <- paste0(getwd(), "/")
out
dir.create("data/cses-pdfs", recursive = TRUE)
out <- paste0(getwd(), "/")
out
dir.create("data/cses-pdfs", recursive = TRUE)
out <- "data/cses-pdfs/"
library(rvest)
page_links <- read_html(url) %>% html_nodes("a") %>% html_attr("href")
survey_pdfs <- str_subset(page_links, "/survey")
# set up folder data/cses-pdfs.
dir.create("data/cses-pdfs", recursive = TRUE)
# download a sample of 10 of the survey questionnaire PDFs into that folder using a for loop and the download.file() function.
baseurl <- "http://www.cses.org"
for (i in 1:10) {
filename <- basename(survey_pdfs[i])
if(!file.exists(paste0("data/cses-pdfs/", filename))){
download.file(paste0(baseurl, survey_pdfs[i]), destfile = paste0("data/cses-pdfs/", filename))
Sys.sleep(runif(1, 0, 1))
}
}
length(list.files(out))
list.files(out)
file_info <- file.info(out)
file_info$size
file_info
file_info <- file.info(dir(out))
file_info
file_info$size
file_info <- file.info(dir(out), full.names== TRUE)
file_info <- file.info(dir(out), full.names= TRUE)
file_info$size
file_info <- file.info(dir("data/cses-pdfs/"), full.names= TRUE)
file_info$size
file_info <- file.info(dir("data/cses-pdfs/"), full.names= TRUE) %>% View
zip("./data/cses-pdfs/zip_cses.pdfs.zip", dir("data/cses-pdfs/", full.names = T))
url <- url("https://books.google.de/books?id=-fgYAAAAQBAJ&printsec=frontcover&dq=argumentationsmuster+deliberation&hl=en&sa=X&redir_esc=y#v=onepage&q=argumentationsmuster%20deliberation&f=false")
info.url(url)
file_info(url)
file.info(url)
dirname(url)
url <- url("https://books.google.de/books?id=-fgYAAAAQBAJ&printsec=frontcover&dq=argumentationsmuster+deliberation&hl=en&sa=X&redir_esc=y#v=onepage&q=argumentationsmuster%20deliberation&f=false")
url <- "https://books.google.de/books?id=-fgYAAAAQBAJ&printsec=frontcover&dq=argumentationsmuster+deliberation&hl=en&sa=X&redir_esc=y#v=onepage&q=argumentationsmuster%20deliberation&f=false"
file.info(url)
dirname(url)
file_inf <- file.info(dir(recursive = F))
file_inf
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
str_extract(example.obj, "small")
str_extract(example.obj, "banana")
?grep()
str_extract_all(example.obj, "sentence")
unlist(str_extract_all(example.obj, "sentence"))
grep("small", example.obj)
grep("small", example.obj, text)
unlist(str_extract_all(example.obj, "sentence", value= T))
unlist(str_extract_all(example.obj, "sentence", value= TRUE))
grep("small", example.obj, text, value= TRUE)
str_extract(example.obj, "sentence")
out <- str_extract_all(c("text", "manipulation", "basics"), "a")
out
str_extract(example.obj, ignore.case("SMALL"))
=======
source("packages.r")
library(stringr)
string1 <- "This is a string"
string1 %>% class()
double_quote <- "\"" # or '"'
double_quote
?"'"
double_quote
writeLines(double_quote) # shows raw contents of the string
cat(double_quote)
x <- "\u00b5"
x
writeLines(x)
x
double_quote
x <- c("apple", "banana", "pear")
x
?str_view
str_view(x, "an")
str_view_all(x, "an")
str_view_all(x, "a")
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
raw.data
name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]{2,}"))
name
phone <- unlist(str_extract_all(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}"))
phone
str_extract_all(raw.data, "[[:alpha:]., ]{2,}")
name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]{2,}"))
name
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
example.obj
str_extract(example.obj, "small")
str_extract(example.obj, "banana")
(out <- str_extract_all(c("text", "manipulation", "basics"), "a"))
c("text", "manipulation", "basics"), "a"
c("text", "manipulation", "basics")
(out <- str_extract_all(c("text", "manipulation", "basics"), "a"))
str_extract(example.obj, "small")
str_extract(example.obj, "SMALL")
str_extract(example.obj, ignore.case("SMALL")) # wrong
str_extract(example.obj, regex("SMALL", ignore_case = TRUE))
example.obj
str_extract(example.obj, "mall sent")
str_extract(example.obj, "^1")
>>>>>>> simonmunzert/master
str_extract(example.obj, "^2")
str_extract(example.obj, "sentence$")
str_extract(example.obj, "sentence.$")
unlist(str_extract_all(example.obj, "tiny|sentence"))
str_extract(example.obj, "sm.ll")
<<<<<<< HEAD
str_extract(example.obj, "sm[abs]ll")
str_extract(example.obj, "sm[abs]ll")
str_extract(example.obj, "sm[a-p]")
[:digit:]   # digits 0-9
t <-      [:digit:]   # digits 0-9
str_extract_all(example.obj, [[:punct]])
str_extract_all(example.obj, [[:punct:]])
str_extract_all(example.obj, "[[:punct:]]")
str_extract_all("François Hollande", "Fran[:alpha:]oise") # returns NA
str_extract_all("François Hollande", "Fran[[:alpha:]]oise") # returns NA
str_extract_all("François Hollande", "Fran[[:alpha:]]ois") # returns NA
str_extract_all(example.obj, "[[:punct:]ABC]")
str_extract_all(example.obj, "[^[:alnum:]]")
str_extract(example.obj, "s[[:alnum:]][[:alnum:]][[:alnum:]]l")
str_extract(example.obj, "s...l")
=======
str_extract(example.obj, "sm[abc]ll")
str_extract(example.obj, "sm[a-p]ll")
unlist(str_extract_all(example.obj, "[uvw. ]"))
example.obj
unlist(str_extract_all(example.obj, "[:punct:]"))
?base::regex
unlist(str_extract_all(example.obj, "[[:punct:]ABC]"))
unlist(str_extract_all(example.obj, "[^[:alnum:]]"))
str_extract(example.obj, "s[[:alpha:]][[:alpha:]][[:alpha:]]l")
str_extract(example.obj, "s[[:alpha:]]{3}l")
str_extract(example.obj, "A.+sentence")
str_extract(example.obj, "A.+?sentence")
unlist(str_extract_all(example.obj, "(.en){1,5}"))
unlist(str_extract_all(example.obj, ".en{1,5}"))
unlist(str_extract_all(example.obj, "(.en){1,4}"))
unlist(str_extract_all(example.obj, "(.en){1,3}"))
unlist(str_extract_all(example.obj, "(.en)"))
unlist(str_extract_all(example.obj, "(.en){1,1}"))
unlist(str_extract_all(example.obj, "(.en){1,5}"))
unlist(str_extract_all(example.obj, "(.en){2,5}"))
unlist(str_extract_all(example.obj, "(.en){1,5}"))
unlist(str_extract_all(example.obj, "(.en){1,6}"))
unlist(str_extract_all(example.obj, "(.en){1,7}"))
unlist(str_extract_all(example.obj, "(.en){1,3}"))
example.obj
unlist(str_extract_all(example.obj, "\\."))
unlist(str_extract_all(example.obj, fixed(".")))
unlist(str_extract_all(example.obj, fixed("sentence.")))
unlist(str_extract_all(example.obj, "[12-]"))
unlist(str_extract_all(example.obj, "[1-2]"))
example.obj
str_extract(example.obj, "([[:alpha:]]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
unlist(str_extract_all(example.obj, "(?<=2. ).+")) # positive lookbehind: (?<=...)
unlist(str_extract_all(example.obj, ".+(?=2)")) # positive lookahead (?=...)
unlist(str_extract_all(example.obj, "(?<!Blah )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Bla)")) # negative lookahead (?!...)
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
<<<<<<< HEAD
>>>>>>> simonmunzert/master
=======
source("packages.r")
source("functions.r")
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
example.obj
str_locate(example.obj, "tiny")
str_sub(example.obj, start = 35, end = 38)
str_sub(example.obj, 35, 38) <- "huge"
str_sub(example.obj, 35, 38) <- "huge"
example.obj
str_replace(example.obj, pattern = "huge", replacement = "giant")
str_split(example.obj, "-")
str_split(example.obj, "-") %>% unlist
str_split_fixed(example.obj, "[[:blank:]]", 5) %>% as.character()
(char.vec <- c("this", "and this", "and that"))
char.vec
str_detect(char.vec, "this")
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
str_dup(char.vec, 3)
length.char.vec <- str_length(char.vec)
char.vec
length.char.vec <- str_length(char.vec)
length.char.vec
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
?str_trim
str_c("text", "manipulation", sep = " ")
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c(char.vec, collapse = "\n")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
?agrep
agrepl("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
agrepl("Donald Trump", "Barack Obama", max.distance = list(all = 3))
library(stringi)
example.obj
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[humboldt]")
browseURL("https://www.nytimes.com/")
browseURL("http://flukeout.github.io/") # let's play this together until plate 8 or so!
source("packages.r")
email <- "chunkylover53[at]aol[dot]com"
email_new <- email %>% str_replace("\\[at\\]", "@") %>% str_replace("\\[dot\\]", ".")
email_new
str_extract(email_new, "[:digit:]+")
regex <- ".*"
string <- c("1. This is an example string by", "2. Eddie (born 1961 in München)", "!§%$&/)(}")
str_extract_all(string, regex)
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
(name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]{2,}")))
(name_sorted <-  str_replace(name, "(.+), (.+)", "\\2 \\1"))
has_title <- str_detect(name, pattern="Dr\\. |Rev\\.")
has_title
name_elements <- str_count(name_sorted, "\\w+")
has_second_name <- ifelse(has_title == FALSE & name_elements > 2, TRUE,
ifelse(has_title == TRUE & name_elements > 3, TRUE, FALSE))
has_second_name
string <- "<title>+++BREAKING NEWS+++</title>"
str_extract(string, "<.+>")
str_extract(string, "<.+?>")
string <- "(5-3)^2=5^2-2*5*3+3^2 conforms to the binomial theorem"
regex <- "[^0-9=+*()]+"
str_extract_all(string, regex)
regex_correct <- "[[:digit:][:punct:]]+"
regex_correct_alt <- "[[:digit:]()=*-^]+"
=======
>>>>>>> simonmunzert/master
regex_correct
str_extract_all(string, regex_correct)
str_extract_all(string, regex_correct_alt)
secret <- "clcopCow1zmstc0d87wnkig7OvdicpNuggvhryn92Gjuwczi8hqrfpRxs5Aj5dwpn0TanwoUwisdij7Lj8kpf03AT5Idr3coc0bt7yczjatOaootj55t3Nj3ne6c4Sfek.r1w1YwwojigOd6vrfUrbz2.2bkAnbhzgv4R9i05zEcrop.wAgnb.SqoU65fPa1otfb7wEm24k6t3sR9zqe5fy89n6Nd5t9kc4fE905gmc4Rgxo5nhDk!gr"
(solved <- unlist(str_extract_all(secret, "[[:upper:][:punct:]]")))
str_c(solved, collapse="")
browseURL("https://www.jstatsoft.org/about/editorialTeam")
<<<<<<< HEAD
>>>>>>> simonmunzert/master
=======
browseURL("http://www.whoishostingthis.com/tools/user-agent/")
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
session <- html_session("http://www.google.com", user_agent(uastring))
search <- html_form(session)[[1]]
library(rvest)
url <- "http://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
names(tables)
mps <- tables[[4]]
head(mps)
# clean up
head(mps)
names(mps) <- c("con", "name", "party")
mps <- mps[-1,]
mps <- mps[!is.na(as.character(mps$party)),]
nrow(mps)
# look for Sirs
mps$name <- as.character(mps$name)
mps$sir <- str_detect(mps$name, "^Sir ")
table(mps$party, mps$sir)
prop.table(table(mps$party, mps$sir), 1)
library(stringr)
mps$name <- as.character(mps$name)
mps$sir <- str_detect(mps$name, "^Sir ")
table(mps$party, mps$sir)
mps <- tables[[4]]
head(mps)
head(mps)
names(mps) <- c("con", "name", "party")
mps <- mps[-1,]
tables <- html_table(url_parsed, fill = FALSE)
tables <- html_table(url_parsed, fill = TRUE)
names(tables)
mps <- tables[[4]]
head(mps)
mps <- filter(mps, !str_detect("[edit"))
mps <- filter(mps, !str_detect(X2, "[edit"))
mps <- mps[!str_detect(mps$X1, "[edit"),]
mps <- mps[!str_detect(mps$X1, "\\[edit"),]
nrow(mps)
# look for Sirs
mps$name <- as.character(mps$name)
mps$sir <- str_detect(mps$name, "^Sir ")
table(mps$party, mps$sir)
prop.table(table(mps$party, mps$sir), 1)
mps$name <- as.character(mps$name)
head(mps)
tables <- html_table(url_parsed, fill = TRUE)
names(tables)
mps <- tables[[4]]
head(mps)
head(mps)
names(mps) <- c("con", "name", "party")
mps <- mps[!str_detect(mps$X1, "\\[edit"),]
mps <- mps[-1,]
nrow(mps)
str_detect(mps$X1, "\\[edit")
str_detect(mps$X1, "\\[edit")
head(mps)
names(mps) <- c("con", "name", "party")
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
names(tables)
mps <- tables[[4]]
head(mps)
# clean up
head(mps)
names(mps) <- c("con", "name", "party")
mps <- mps[str_detect(mps$con, "\\[edit"),]
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
names(tables)
mps <- tables[[4]]
head(mps)
# clean up
head(mps)
names(mps) <- c("con", "name", "party")
mps <- mps[!str_detect(mps$con, "\\[edit"),]
head(mps)
mps <- mps[-1,]
nrow(mps)
# look for Sirs
mps$name <- as.character(mps$name)
mps$sir <- str_detect(mps$name, "^Sir ")
table(mps$party, mps$sir)
prop.table(table(mps$party, mps$sir), 1)
<<<<<<< HEAD
>>>>>>> simonmunzert/master
=======
812.76*0.94195
(7.50+63.35)*0.94195
(7.50+63.35)*0.91466
765.58+580.07+64.8
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl list")
system("launchctl list")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl list")
system("launchctl unload ~/Library/LaunchAgents/scraperspiegelonline.plist")
system("launchctl stop scraperspiegelonline")
system("launchctl unload ~/Library/LaunchAgents/scraper_spiegel_online.plist")
system("launchctl list")
library(stringr)
library(magrittr)
library(httr)
Sys.time()
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
datetime
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl list")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
source("00-course-setup.r")
source("packages.r")
source("packages.r")
source("functions.r")
starships <- get_all_starships()
library(rwars)
starships <- get_all_starships()
foo <- plyr::rbind.fill(starships$results)
starships$results
foo <- lapply(starships$results, as.data.frame)
starships$results
browseURL("https://github.com/ropensci/opendata")
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
ip_df <- geolocate(c(NA, "10.0.1.1", "", "72.33.67.89", "dds.ec", " ", "search.twitter.com"), .progress=FALSE)
ip_df
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "spiegel.de", "search.twitter.com"), .progress=FALSE)
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "spiegel.de", "search.twitter.com"), .progress=FALSE)
?geolocate
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
url <- "http://ip-api.com/xml/"
read_xml(url)
?trimws
ip_parsed <- read_xml(url)
xml_find_all(ip_parsed, "//query")
data.frame(ip_parsed)
xmlToDataFrame(ip_parsed)
library(XML)
xmlToDataFrame(ip_parsed)
as_list(ip_parsed)
ip_list <- as_list(ip_parsed)
ip_list
str(ip_list)
unlist(ip_list)
ip_list %>% unlist %>% as.data.frame(stringsAsFactors = FALSE)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
url <- "http://ip-api.com/json/"
ip_parsed <- fromJSON(url)
url <- "http://ip-api.com/json"
ip_parsed <- fromJSON(url)
?fromJSON
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
?fromJSON
ip_parsed <- jsonlite::fromJSON(url, flatten = TRUE)
ip_parsed
ip_parsed <- jsonlite::fromJSON(url, simplifyDataFrame = TRUE, flatten = TRUE)
ip_parsed
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
install.packages("translate")
install.packages("gdap")
install.packages("diezeit")
library(diezeit)
api_key <- "f9bd321bca869ff43311a8e83706413b82a099da993a41560640"
?zeit_search
foo <- zeit_search(endpoint="content", query="bayreuth")
str(foo)
length(foo)
author <- zeit_search(endpoint="author", query="stefan*locke")
author <- zeit_search(endpoint="author", query="johanna*haag")
author <- zeit_search(endpoint="author", query="tobias*landwehr")
author <- zeit_search(endpoint="keyword", query="wiedervereinigung")
author <- zeit_search(endpoint="series", query="deutschlandkarte")
series <- zeit_search(endpoint="series", query="deutschlandkarte")
zeit_get("content", "3Ed7KYJOO2MXu5SQtnudQA")
meta <- zeit_get("content", "3Ed7KYJOO2MXu5SQtnudQA")
merkel <- zeit_search(endpoint="content", query="title:Merkel")
str(merkel)
unlist(merkel)
foo <- zeit_search(endpoint="content", query="bayreuth")
names(foo)
dat <- zeit_search(endpoint="content", query="bayreuth")
names(dat)
dat$matches
dat$matches %>% unlist
str(dat$matches)
dd  <-  as.data.frame(matrix(unlist(dat$matches), nrow=length(dat$matches(listHolder[1]))))
length(dat$matches(listHolder[1])
dd  <-  as.data.frame(matrix(unlist(dat$matches), nrow=length(dat$matches(length(dat$matches(listHolder[1])[1]))))
foo <- zeit_search(endpoint="series", query="bayreuth")
author <- zeit_search(endpoint="author", query="tobias*landwehr")
dd  <-  as.data.frame(matrix(unlist(dat$matches), nrow=length(dat$matches(length(dat$matches(listHolder[1])[1])))))
dd  <-  as.data.frame(matrix(unlist(dat$matches),
nrow=length(unlist(dat$matches[1]))))
View(dd)
do.call(rbind, dat$matches).
do.call(rbind, dat$matches)
dd <- do.call(rbind, dat$matches)
View(dat$matches)
View(dd)
View(dd)
dd$snippelt
dd$snippet
names(dd)
class(dd)
dd <- do.call(rbind, dat$matches) %>% as.data.frame(stringsAsFactors = FALSE)
View(dd)
dd$snippet
sapply(dd, class)
df= as.data.frame(t(as.data.frame(dat$matches)))
View()
View(df)
as.data.frame(dat$matches)
do.call(c, unlist(dat$matches, recursive=FALSE))
sapply(dd, as.vector)
dd <- do.call(rbind, dat$matches) %>% as.data.frame(stringsAsFactors = FALSE)
View(dd)
sapply(dd, class)
sapply(dd, unlist)
ddd <- sapply(dd, unlist)
class(ddd)
ddd <- sapply(dd, unlist) %>% as.data.frame(stringsAsFactors = FALSE)
sapply(ddd, class)
View(ddd)
dd <- do.call(rbind, dat$matches) %>% as.data.frame(stringsAsFactors = FALSE) %>% sapply(unlist) %>% as.data.frame(stringsAsFactors = FALSE)
View(dd)
dd$snippet
foo <- zeit_search(endpoint="content", query="merkel")
merkel$found
merkel <- zeit_search(endpoint="content", query='"Kennedy" AND release_date:[1960-01-01T00:00:00Z TO 1969-12-31T23:59:59.999Z]')
meta <- zeit_get("content", "3Ed7KYJOO2MXu5SQtnudQA")
meta <- zeit_get("content", "Genug Liebe für mehr als zwei")
meta <- zeit_get("content", query = "Und Recht und Freibier", fields = "title")
meta <- zeit_get("content", "Und Recht und Freibier", fields = "title")
zeit_get
meta <- zeit_get("content", id = "Und Recht und Freibier", fields = "title")
zeit_client(print = TRUE)
?zeit_get
?zeit_search
?zeit_get
meta <- zeit_get("content", "3Ed7KYJOO2MXu5SQtnudQA")
<<<<<<< HEAD
>>>>>>> simonmunzert/master
=======
source("packages.r")
source("functions.r")
library(diezeit)
merkel <- zeit_search(endpoint="content", query="merkel")
browseURL("http://ip-api.com/")
title <- "Groundhog Day" %>% URLencode()
endpoint <- "http://www.omdbapi.com/?"
url <- paste0(endpoint, "t=", title, "&tomatoes=true")
omdb_res = GET(url)
res_list <- content(omdb_res, as =  "parsed")
=======
>>>>>>> simonmunzert/master
res_list %>% unlist() %>% t() %>% data.frame(stringsAsFactors = FALSE)
browseURL("http://www.omdbapi.com/")
browseURL("http://openweathermap.org/current")
library(newsAPI)
NEWSAPI_KEY <- "dd893f503fc24af2b4a47889244eaf27"
## save to .Renviron file
cat(
paste0("NEWSAPI_KEY=", NEWSAPI_KEY),
append = TRUE,
fill = TRUE,
file = file.path("~", ".Renviron")
)
src <- get_sources(language = "en")
src
src <- get_sources(language = "de")
src
df <- lapply(src$id, get_articles)
?get_articles
src <- get_sources(language = "en")
df <- lapply(src$id, get_articles)
get_articles("espn")
get_articles
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY)
df
df <- do.call("rbind", df)
View(df)
src <- get_sources(language = "de")
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY)
df <- do.call("rbind", df)
View(df)
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY, sortBy = "latest")
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY, sortBy = "top")
df <- lapply(src$id, get_articles, apiKey = NEWSAPI_KEY)
df <- do.call("rbind", df)
View(df)
get_articles("bild", apiKey = NEWSAPI_KEY)
source("packages.r")
source("functions.r")
browseURL("http://httpbin.org")
GET("http://httpbin.org/headers")
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com",
`User-Agent` = R.Version()$version.string))
url_response <- GET("http://spiegel.de/schlagzeilen",
add_headers(From = "my@email.com"))
url_parsed <- url_response  %>% read_html()
url_parsed %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
url <- "http://spiegel.de/schlagzeilen"
session <- html_session(url, add_headers(From = "my@email.com"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
?html_session
browseURL("https://www.google.com/robots.txt")
browseURL("http://www.nytimes.com/robots.txt")
library(robotstxt)
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "Twitterbot")
browseURL("http://httpbin.org")
source("packages.r")
GET("http://httpbin.org/headers")
GET("http://httpbin.org/headers")
R.Version()$version.string
GET("http://httpbin.org/headers", add_headers(`User-Agent` = R.Version()$version.string))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
GET("http://httpbin.org/headers", add_headers(Hello = "my@email.com"))
browseURL("https://www.google.com/robots.txt")
library(robotstxt)
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/", "http://facebook.com/", bot = "*")
paths_allowed("/", "https://facebook.com/", bot = "*")
?paths_allowed
paths_allowed("/imgres", "http://google.com/", bot = "*")
paths_allowed("/imgres", "http://google.com/", bot = "Twitterbot")
source("packages.r")
source("functions.r")
browseURL("http://ropensci.org/")
browseURL("https://github.com/ropensci/opendata")
browseURL("https://cran.r-project.org/web/views/WebTechnologies.html")
browseURL("http://ip-api.com/")
library(ipapi)
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
system("java -version")
rD <- rsDriver()
source("packages.r")
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/", "https://facebook.com/", bot = "*")
browseURL("https://mkearney.github.io/rtweet/articles/auth.html")
library(rtweet)
install.packages("rtweet")
load("/Users/munzerts/rkeys.RDa")
key <- TwitterToR_twitterkey
secret <- TwitterToR_twittersecret
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
library(rtweet)
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
appname <- "TwitterToR"
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
install.packages("httpuv")
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
q <- paste0("schulz,merkel,btw17,btw2017")
?stream_tweets
twitter_stream_ger <- stream_tweets(q = q, timeout = 30, token = twitter_token)
twitter_stream_ger
class(twitter_stream_ger)
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "btw17"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
filename
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
stream_tweets(q = q, parse = FALSE,
timeout = 30,
file_name = filename,
token = twitter_token)
rt <- parse_stream(filename)
names(rt)
head(rt)
View(rt)
users_data(rt) %>% head()
users_data(rt) %>% names()
# set up directory and JSON dump
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "btw17"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
# create file with stream's meta data
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
60*60
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 3600,
file_name = filename,
token = twitter_token)
rt <- parse_stream(filename)
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 3600,
file_name = filename,
language = "de",
token = twitter_token)
source("packages.r")
browseURL("https://mkearney.github.io/rtweet/articles/auth.html")
1500*0.5*.5*5
1500*.5*.5*5
30/80
30/66
1700*.6*.5*5
1700*.15*.5*5
library(rtweet)
appname <- "TwitterToR"
load("/Users/munzerts/rkeys.RDa")
key <- TwitterToR_twitterkey
secret <- TwitterToR_twittersecret
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
?search_tweets
ts_plot(rt, by = "mins", theme = "spacegray",
main = "Tweets about Trump")
rt <- search_tweets("merkel", n = 2000, include_rts = TRUE, token = twitter_token)
names(rt)
View(rt)
rt <- search_tweets("merkel", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
names(rt)
ts_plot(rt, by = "mins", theme = "spacegray",
main = "Tweets about Merkel")
rt$created_at
?ts_plot
ts_plot(rt, by = "mins", theme = "spacegray", main = "Tweets about Merkel")
ts_plot(rt, by = "days", theme = "spacegray", main = "Tweets about Merkel")
ts_plot(rt, by = "hours", theme = "spacegray", main = "Tweets about Merkel")
?search_tweets
rt <- search_tweets("merkel :(", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
names(rt)
View(rt)
rt$text
rt <- search_tweets("tauber :(", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
names(rt)
View(rt)
rt$text
rt <- search_tweets(URLEncode("tauber :("), n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
rt <- search_tweets(URLencode("tauber :("), n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
names(rt)
View(rt)
rt <- search_tweets(URLencode("tauber :("), n = 1000, include_rts = FALSE, lang = "de", token = twitter_token)
names(rt)
View(rt)
rt$text
tauber_good <- search_tweets(URLencode("tauber :("), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_good$text
search_tweets
.search_tweets
getAnywhere(".search_tweets")
?make_url
getAnywhere("make_url")
tauber_good <- search_tweets(URLencode("tauber filter:images"), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_good
tauber_good$text
<<<<<<< HEAD
>>>>>>> simonmunzert/master
=======
source("packages.r")
source("functions.r")
browseURL("http://ip-api.com/docs/")
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
url <- "http://ip-api.com/json/72.33.67.89"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.spiegel.de") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
browseURL("http://ip-api.com/docs/")
source("packages.r")
source("functions.r")
ip_parsed <- xml2::read_xml(url)
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_parsed
?as_list
ip_list <- as_list(ip_parsed)
ip_list
ip_list %>% unlist
ip_list %>% unlist %>% t
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
class(ip_parsed)
ip_parsed %>% unlist
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.spiegel.de") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
source("packages.r")
source("functions.r")
browseURL("https://dev.twitter.com/rest/public/search")
appname <- "TwitterToR"
load("/Users/munzerts/rkeys.RDa")
load("/Users/munzerts/rkeys.RDa")
key <- TwitterToR_twitterkey
secret <- TwitterToR_twittersecret
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
?search_tweets
rt <- search_tweets("merkel", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
dim(rt)
names(rt)
View(rt)
URLencode("tauber :(")
tauber_bad <- search_tweets(URLencode("tauber :("), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_bad$text
tauber_good <- search_tweets(URLencode("tauber :)"), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_good$text
tauber_good <- search_tweets(URLencode("tauber filter:images"), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_good$text
ts_plot(rt, by = "hours", theme = "spacegray", main = "Tweets about Merkel")
q <- paste0("schulz,merkel,btw17,btw2017")
twitter_stream_ger <- stream_tweets(q = q, timeout = 30, token = twitter_token)
twitter_stream_ger
View(twitter_stream_ger)
rt <- parse_stream("data/rtweet-data/btw17_2017-07-03-13-02-52.json")
merkel <- str_detect(rt$text, regex("merkel", ignore_case = TRUE))
schulz <- str_detect(rt$text, regex("schulz", ignore_case = TRUE))
mentions_df <- data.frame(merkel,schulz)
colMeans(mentions_df, na.rm = TRUE)
lookup_users
user_df <- lookup_users("RDataCollection")
names(user_df)
user_timeline_df <- get_timeline("RDataCollection")
names(user_timeline_df)
user_timeline_df$text
user_favorites_df <- get_favorites("RDataCollection")
browseURL("http://pablobarbera.com/big-data-upf/")
157.77+22.95+22.95+2.4
36.8+25.9+8.9+8.1+8+8+4.3
source("packages.r")
source("functions.r")
library("WikipediR")
library("WikidataR")
library("pageviews")
library("statsgrokse")
links <- page_links("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 1000)
unlist(links) %>% as.character()
?str_subset
unlist(links) %>% as.character() %>% str_subset("[^0]")
str(wp_content)
wp_content <- page_content("de","wikipedia", page_name = "Brandenburger_Tor")
content <- page_content("de","wikipedia", page_name = "Brandenburger_Tor")
str(content)
content$text
content$parse$text
content$parse$text %>% class
?page_content
content <- page_content("de","wikipedia", page_name = "Brandenburger_Tor", as_wikitext = TRUE)
str(content)
content$parse$text
content$parse$wikitext
?page_backlinks
backlinks <- page_backlinkslinks("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 1000)
backlinks <- page_backlinks("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 1000)
backlinks
unlist(backlinks) %>% as.character() %>% str_subset("[^0]")
backlinks <- page_backlinks("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500, namespaces = 0)
unlist(backlinks) %>% as.character() %>% str_subset("[^0]")
links <- page_links("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500, namespaces = 0)
unlist(links) %>% as.character() %>% str_subset("[^0]")
backlinks <- page_backlinks("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500, namespaces = 0)
backlinks
unlist(backlinks)
unlist(backlinks) %>% names()
backlinks[names(backlinks) == "title"]
unlist(backlinks) %>% as.character() %>% .[names(.) == "title"]
unlist(backlinks) %>%  .[names(.) == "title"]
unlist(backlinks) %>%  .[names(.) == "title"] %>% as.character
?page_external_links
externallinks <- page_external_links("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500, namespaces = 0)
externallinks <- page_external_links("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500)
externallinks
?page_info
pageinfo <- page_info("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE, limit = 500, namespaces = 0)
pageinfo <- page_info("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE)
pageinfo
str(pageinfo)
categories_in_page("de","wikipedia", page = "Brandenburger_Tor", clean_response = TRUE)
?find_item
?find_item
wd_item <- find_item("Brandenburger Tor", language = "de")
wd_item
get_item("82425")
tor_item <- get_item("82425")
wd_item
str(wd_item)
class(v)
class(wd_item)
wd_item <- find_item("Brandenburger Tor", language = "de")
class(wd_item)
str(wd_item)
wd_item[[¡]]
wd_item[[1
wd_item[[1]]$id
tor_item <- get_item("82425")
tor_item
extract_claims(tor_item)
?extract_claims
tor_item
str(tor_item)
names(tor_item)
tor_item$items
tor_item$claims
str(tor_item)
unlist(tor_item)
str(v)
str(tor_item)
tor_item[[1]]
str(tor_item)
unlist(tor_item) %>% names
names_item <- unlist(tor_item) %>% names
tor_item$type
tor_item$id
tor_item$labels
str(tor_item)
names_item <- unlist(tor_item) %>% names
names_item
extract_claims(tor_item, "P31")
extract_claims(tor_item, "P17")
extract_claims(tor_item, "P17")
get_property(tor_item, "P17")
?get_property
get_property(tor_item, "P40")
tor_item <- get_item("82425")
get_property("P40")
get_property("P17")
extract_claims(tor_item, "P17")
get_property("Q183")
extract_claims(tor_item, "P17")
get_property("Q183")
property <- get_property("Q183")
str(property)
property <- get_property("Q183")
property
# search Wikipedia with search term
searchWikiFun <- function(term = NULL, limit = 100, title.only = TRUE, wordcount.min = 500) {
# API doc at https://www.mediawiki.org/wiki/API:Search
term <- URLencode(term)
url <- sprintf("https://de.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&srlimit=%d&format=json", term, limit)
wiki_search_parsed <- jsonlite::fromJSON(url)$query$search
wiki_search_parsed <- dplyr::filter(wiki_search_parsed, wordcount >= wordcount.min)
if(title.only == FALSE) {
return(wiki_search_parsed)
} else{
return(wiki_search_parsed$title)
}
}
# search for categories and pages within these categories
searchWikiCatsFun <- function(pages = NULL, categories = NULL, language = "de", project = "wikipedia", limit = 100, output = c("pages", "cats"), subcats = FALSE, max.numcats = 5) {
if(!is.null(pages)) {
# search categories
cats <- llply(pages, function(x) { try(categories_in_page(language, project, pages = x, clean_response = TRUE, limit = limit)[[1]]$categories$title, silent = TRUE)   }) %>% unlist %>% table %>% sort(decreasing = TRUE) %>% extract(1:max.numcats) %>% names() %>% extract(!str_detect(., "^Kategorie:Wikipedia:Lesenswert")) %>% unique %>% extract(str_detect(., "^Kategorie")) %>% str_replace("Kategorie:", "")
# identify subcategories in categories
if(subcats == TRUE){
subcats <- llply(cats, function(x) { try(pages_in_category(language, project, categories = x, type = "subcat", limit = limit, clean_response = TRUE)$title, silent = TRUE)   }) %>% unlist %>% unique
subcats <- subcats[str_detect(subcats, "^Kategorie")] %>% str_replace("Kategorie:", "")
# combine categories
cats_all <- c(cats, subcats) %>% unique
}else{
cats_all <- cats
}
}else{
cats_all <- categories
}
# find pages in all categories
pages_in_cats <- llply(cats_all, function(x) { try(pages_in_category(language, project, categories = x, type = "page", limit = limit, clean_response = TRUE)$title, silent = TRUE)   }) %>% unlist %>% unique
# return output
if(output == "pages") {
return(pages_in_cats)
}else{
return(cats_all)
}
}
pages_search <- searchWikiFun(term = "Arbeitslosigkeit", limit = 100, wordcount.min = 500)
pages_search
(cats_search <- searchWikiCatsFun(pages = pages_search, output = "cats", subcats = TRUE, max.numcats = 10))
# quick assessment of pages' recent pageview statistics
pagesMinPageviews <- function(pages = NULL, start = "2016090100", end = "2016093000", min.dailyviewsavg = 50) {
pageviews_list <- list()
pageviews_mean <- numeric()
for (i in seq_along(pages)) {
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article = URLencode(pages[i]), start = start, end = end, reformat = TRUE))
pageviews_mean[i] <- try(mean(pageviews_list[[i]]$views, na.rm = TRUE))
}
pageviews_mean_df <- data.frame(page = pages, pageviews_mean = num(pageviews_mean), stringsAsFactors = FALSE)
pages_minviews <- dplyr::filter(pageviews_mean_df, pageviews_mean >= min.dailyviewsavg) %>% extract2("page")
return(pages_minviews)
}
# download pageview statistics with wikipediatrend package
pageviewsDownload <- function(pages = NULL, folder = "~", from = "2008-01-01", to = "2013-12-31", language = "de") {
pageviews_list <- list()
pageviews_filenames_raw <- vector()
for (i in seq_along(pages)) {
filename <- paste0("wp_", pages[i], "_", language, ".csv")
if (!file.exists(paste0(folder, filename))) {
pageviews_list[[i]] <- try(wp_trend(URLencode(pages[i]), from = from, to = to, lang = language))
try(write.csv(pageviews_list[[i]], file = paste0(folder, filename), row.names = FALSE))
}
pageviews_filenames_raw[i] <- filename
}
}
pagenames_arbeitslosigkeit <- pagesMinPageviews(pages = pages_search, start = "2016090100", end = "2016093000", min.dailyviewsavg = 50)
pagenames_arbeitslosigkeit
dir.create("data/wikipageviews")
pageviewsDownload(pages = pagenames_arbeitslosigkeit, folder = "data/wikipageviews/", from = "2008-01-01", to = "2016-01-01", language = "de")
source("packages.r")
pageviewsDownload(pages = pagenames_arbeitslosigkeit, folder = "data/wikipageviews/", from = "2008-01-01", to = "2016-01-01", language = "de")
library(statsgrokse)
?statsgrokse
# download pageview statistics with wikipediatrend package
pageviewsDownload <- function(pages = NULL, folder = "~", from = "2008-01-01", to = "2013-12-31", language = "de") {
pageviews_list <- list()
pageviews_filenames_raw <- vector()
for (i in seq_along(pages)) {
filename <- paste0("wp_", pages[i], "_", language, ".csv")
if (!file.exists(paste0(folder, filename))) {
pageviews_list[[i]] <- try(statsgrokse(URLencode(pages[i]), from = from, to = to, lang = language))
try(write.csv(pageviews_list[[i]], file = paste0(folder, filename), row.names = FALSE))
}
pageviews_filenames_raw[i] <- filename
}
}
pageviewsDownload(pages = pagenames_arbeitslosigkeit, folder = "data/wikipageviews/", from = "2008-01-01", to = "2016-01-01", language = "de")
# download pageview statistics with wikipediatrend package
pageviewsDownload <- function(pages = NULL, folder = "~", from = "2015070100", to = "2017040100", language = "de") {
pageviews_list <- list()
pageviews_filenames_raw <- vector()
for (i in seq_along(pages)) {
filename <- paste0("wp_", pages[i], "_", language, ".csv")
if (!file.exists(paste0(folder, filename))) {
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article =  URLencode(pages[i]), start = from, end = to))
try(write.csv(pageviews_list[[i]], file = paste0(folder, filename), row.names = FALSE))
}
pageviews_filenames_raw[i] <- filename
}
}
pageviewsDownload(pages = pagenames_arbeitslosigkeit, folder = "data/wikipageviews/", from = "2015070100", to = "2017040100", language = "de")
>>>>>>> simonmunzert/master
